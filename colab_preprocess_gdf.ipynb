{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e0baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mne torch transformers scikit-learn scipy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4cca882",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:7f300a8b-b03c-459b-ac26-151d6b47e7db"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    " EEGEncoder - Colab Data Preprocessing (Using .mat files ‚Äî same as original)\n",
    "=============================================================================\n",
    " PURPOSE: This is the original preprocess.py adapted for Google Colab.\n",
    "          Uses the EXACT SAME loading logic from the reference paper code.\n",
    "          The only change is making paths configurable.\n",
    "\n",
    " ORIGINAL CODE: EEGEncoder-main/preprocess.py\n",
    " CHANGES:       - Paths are configurable (not hardcoded)\n",
    "                - Added print statements for progress tracking\n",
    "                - No changes to data loading, slicing, or standardization\n",
    "\n",
    " INSTALL: pip install scipy scikit-learn numpy\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "def load_BCI2a_data(data_path, subject, training, all_trials=True):\n",
    "    \"\"\"\n",
    "    Loading and Dividing of the data set based on the subject-specific\n",
    "    (subject-dependent) approach.\n",
    "    \n",
    "    THIS IS THE EXACT SAME FUNCTION FROM THE ORIGINAL preprocess.py.\n",
    "    No modifications ‚Äî ensures results match the reference paper.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : str\n",
    "        Path to the directory containing .mat files.\n",
    "        Must end with '/' and contain subject subdirectories like s1/, s2/, etc.\n",
    "        OR contain files directly (see get_data for path construction).\n",
    "    subject : int\n",
    "        Subject number in [1, .., 9]\n",
    "    training : bool\n",
    "        If True, load training data (A0xT.mat)\n",
    "        If False, load testing data (A0xE.mat)\n",
    "    all_trials : bool\n",
    "        If True, load all trials (including rejected ones)\n",
    "        If False, ignore trials with artifacts\n",
    "    \"\"\"\n",
    "    # Define MI-trials parameters\n",
    "    n_channels = 22\n",
    "    n_tests = 6 * 48     # 288 trials per session\n",
    "    window_Length = 7 * 250  # 1750 samples (7 seconds at 250 Hz)\n",
    "\n",
    "    # Define MI trial window\n",
    "    fs = 250            # sampling rate\n",
    "    t1 = int(1.5 * fs)  # 375 ‚Äî start at 1.5s after trial onset\n",
    "    t2 = int(6 * fs)    # 1500 ‚Äî end at 6.0s after trial onset\n",
    "\n",
    "    class_return = np.zeros(n_tests)\n",
    "    data_return = np.zeros((n_tests, n_channels, window_Length))\n",
    "\n",
    "    NO_valid_trial = 0\n",
    "    if training:\n",
    "        a = sio.loadmat(data_path + 'A0' + str(subject) + 'T.mat')\n",
    "    else:\n",
    "        a = sio.loadmat(data_path + 'A0' + str(subject) + 'E.mat')\n",
    "    a_data = a['data']\n",
    "    for ii in range(0, a_data.size):\n",
    "        a_data1 = a_data[0, ii]\n",
    "        a_data2 = [a_data1[0, 0]]\n",
    "        a_data3 = a_data2[0]\n",
    "        a_X = a_data3[0]\n",
    "        a_trial = a_data3[1]\n",
    "        a_y = a_data3[2]\n",
    "        a_artifacts = a_data3[5]\n",
    "\n",
    "        for trial in range(0, a_trial.size):\n",
    "            if (a_artifacts[trial] != 0 and not all_trials):\n",
    "                continue\n",
    "            data_return[NO_valid_trial, :, :] = np.transpose(\n",
    "                a_X[int(a_trial[trial]):(int(a_trial[trial]) + window_Length), :22]\n",
    "            )\n",
    "            class_return[NO_valid_trial] = int(a_y[trial])\n",
    "            NO_valid_trial += 1\n",
    "\n",
    "    data_return = data_return[0:NO_valid_trial, :, t1:t2]\n",
    "    class_return = class_return[0:NO_valid_trial]\n",
    "    class_return = (class_return - 1).astype(int)\n",
    "\n",
    "    return data_return, class_return\n",
    "\n",
    "\n",
    "def standardize_data(X_train, X_test, channels):\n",
    "    \"\"\"\n",
    "    StandardScaler per channel ‚Äî EXACT COPY from original preprocess.py.\n",
    "    Fit on training data, transform both train and test.\n",
    "    \"\"\"\n",
    "    for j in range(channels):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train[:, 0, j, :])\n",
    "        X_train[:, 0, j, :] = scaler.transform(X_train[:, 0, j, :])\n",
    "        X_test[:, 0, j, :] = scaler.transform(X_test[:, 0, j, :])\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def get_data(path, subject, dataset='BCI2a', n_classes=4, isStandard=True, isShuffle=True):\n",
    "    \"\"\"\n",
    "    Load, split, reshape, standardize ‚Äî EXACT COPY from original preprocess.py.\n",
    "    Only change: path construction uses direct path instead of s{subject}/ subdir.\n",
    "    \"\"\"\n",
    "    X_train, y_train = load_BCI2a_data(path, subject + 1, True)\n",
    "    X_test, y_test = load_BCI2a_data(path, subject + 1, False)\n",
    "\n",
    "    # Shuffle the data\n",
    "    if isShuffle:\n",
    "        X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "        X_test, y_test = shuffle(X_test, y_test, random_state=42)\n",
    "\n",
    "    # Prepare training data\n",
    "    N_tr, N_ch, T = X_train.shape\n",
    "    X_train = X_train.reshape(N_tr, 1, N_ch, T)\n",
    "    y_train_onehot = np.eye(n_classes)[y_train]\n",
    "\n",
    "    # Prepare testing data\n",
    "    N_tr, N_ch, T = X_test.shape\n",
    "    X_test = X_test.reshape(N_tr, 1, N_ch, T)\n",
    "    y_test_onehot = np.eye(n_classes)[y_test]\n",
    "\n",
    "    # Standardize the data\n",
    "    if isStandard:\n",
    "        X_train, X_test = standardize_data(X_train, X_test, N_ch)\n",
    "\n",
    "    return X_train, y_train, y_train_onehot, X_test, y_test, y_test_onehot\n",
    "\n",
    "\n",
    "def data_save(data_path, output_dir, n_sub=9, n_classes=4):\n",
    "    \"\"\"\n",
    "    Main preprocessing function ‚Äî matches original data_save().\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : str\n",
    "        Directory containing .mat files (A01T.mat, A01E.mat, ...).\n",
    "        Must end with '/'.\n",
    "    output_dir : str\n",
    "        Directory to save .pkl files.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for sub in range(n_sub):\n",
    "        print(f\"\\n--- Subject {sub + 1} ---\")\n",
    "\n",
    "        X_train, _, y_train_onehot, X_test, _, y_test_onehot = get_data(\n",
    "            data_path, sub, n_classes=n_classes, isStandard=True)\n",
    "\n",
    "        data_to_save = (X_train, X_test, y_train_onehot, y_test_onehot)\n",
    "        save_path = os.path.join(output_dir, f'data_all_{sub + 1}.pkl')\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(data_to_save, f)\n",
    "\n",
    "        print(f\"  Train: {X_train.shape}  Test: {X_test.shape}\")\n",
    "        print(f\"  ‚úÖ Saved: {save_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # === CONFIGURE THESE PATHS ===\n",
    "    # Point data_path to the directory containing A01T.mat, A01E.mat, etc.\n",
    "    # Mount Google Drive (skip if already mounted)\n",
    "    import os\n",
    "    # Check what's in your Drive folder\n",
    "    base = '/content/drive/MyDrive/eegencoder/'\n",
    "    print(\"Contents of eegencoder/:\")\n",
    "    for item in os.listdir(base):\n",
    "        full = os.path.join(base, item)\n",
    "        if os.path.isdir(full):\n",
    "            print(f\"  üìÅ {item}/\")\n",
    "            for sub in os.listdir(full)[:5]:\n",
    "                print(f\"      {sub}\")\n",
    "        else:\n",
    "            print(f\"  üìÑ {item}\")\n",
    "    from google.colab import drive\n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "    except ValueError:\n",
    "        print(\"Drive already mounted, continuing...\")\n",
    "    # For Google Colab with Drive:\n",
    "    data_path  = '/content/drive/MyDrive/eegencoder/datasets/'\n",
    "    output_dir = '/content/drive/MyDrive/eegencoder/datasets/mat/data/'\n",
    "\n",
    "    # For local:\n",
    "    # data_path  = './EEGEncoder-main/datasets/'\n",
    "    # output_dir = './EEGEncoder-main/data/'\n",
    "\n",
    "    data_save(data_path, output_dir)\n",
    "    print(\"\\n‚úÖ Preprocessing complete! PKL files ready for training.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
