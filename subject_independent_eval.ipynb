{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8057649b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "except:\n",
    "    print(\"Drive already mounted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc15bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/eegencoder/efficient_eegencoder.py /content/\n",
    "!cp /content/drive/MyDrive/eegencoder/lma.py /content/\n",
    "!cp /content/drive/MyDrive/eegencoder/colab_train_baseline.py /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "452e4e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colab_train_baseline.py  efficient_eegencoder.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
      "\u001b[01;34mdrive\u001b[0m/                   lma.py                   \u001b[01;34msample_data\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa8c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat colab_train_baseline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce2517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: colab_kernel_launcher.py [-h] [--model {baseline,efficient}]\n",
      "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-8d9e45ce-c3bc-4a43-b6e0-05399a773163.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    " Subject-Independent Evaluation â€” Leave-One-Subject-Out Cross-Validation\n",
    "=============================================================================\n",
    " PURPOSE: Evaluate subject independence by training on 8 subjects and testing\n",
    "          on the held-out subject. This is the professor's secondary target.\n",
    "\n",
    " USAGE:\n",
    "   python subject_independent_eval.py --model baseline   # Test original\n",
    "   python subject_independent_eval.py --model efficient   # Test proposed\n",
    "\n",
    " PAPER REFERENCE: Subject-Independent accuracy = 74.48%\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "\n",
    "# Ensure imports work from this directory\n",
    "sys.path.insert(0, '/content') \n",
    "\n",
    "\n",
    "# ==================== CONFIG ====================\n",
    "DATA_DIR = '/content/drive/MyDrive/eegencoder/data/'\n",
    "N_SUBJECTS = 9\n",
    "N_CLASSES = 4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "N_EPOCHS = 100       # Fewer epochs since more training data\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "\n",
    "\n",
    "# ==================== DATA LOADING ====================\n",
    "def load_all_subjects(data_dir, n_subjects=9):\n",
    "    \"\"\"Load all subject data from PKL files.\"\"\"\n",
    "    all_train = []\n",
    "    all_test = []\n",
    "    all_train_labels = []\n",
    "    all_test_labels = []\n",
    "    \n",
    "    for sub in range(1, n_subjects + 1):\n",
    "        pkl_path = os.path.join(data_dir, f'data_all_{sub}.pkl')\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            X_train, X_test, y_train_oh, y_test_oh = pickle.load(f)\n",
    "        # Pool train + test from each subject for LOSO\n",
    "        all_train.append(X_train)\n",
    "        all_test.append(X_test)\n",
    "        all_train_labels.append(y_train_oh)\n",
    "        all_test_labels.append(y_test_oh)\n",
    "    \n",
    "    return all_train, all_test, all_train_labels, all_test_labels\n",
    "\n",
    "\n",
    "class CombinedDataset(data.Dataset):\n",
    "    \"\"\"Dataset that combines data from multiple subjects.\"\"\"\n",
    "    def __init__(self, X_list, y_list):\n",
    "        self.x = torch.tensor(np.concatenate(X_list, axis=0)).float()\n",
    "        self.y = torch.tensor(np.concatenate(y_list, axis=0)).float()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "def loso_split(all_train, all_test, all_train_labels, all_test_labels, test_subject_idx):\n",
    "    \"\"\"\n",
    "    Leave-One-Subject-Out split.\n",
    "    \n",
    "    Training: all data from (N-1) subjects (both their train and test sessions)\n",
    "    Testing:  all data from the held-out subject (both train and test sessions)\n",
    "    \"\"\"\n",
    "    train_X, train_y = [], []\n",
    "    test_X, test_y = [], []\n",
    "    \n",
    "    for i in range(len(all_train)):\n",
    "        if i == test_subject_idx:\n",
    "            # This subject is the test subject â€” use ALL their data for testing\n",
    "            test_X.append(all_train[i])\n",
    "            test_X.append(all_test[i])\n",
    "            test_y.append(all_train_labels[i])\n",
    "            test_y.append(all_test_labels[i])\n",
    "        else:\n",
    "            # Pool train + test data from other subjects for training\n",
    "            train_X.append(all_train[i])\n",
    "            train_X.append(all_test[i])\n",
    "            train_y.append(all_train_labels[i])\n",
    "            train_y.append(all_test_labels[i])\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "\n",
    "# ==================== TRAINING ====================\n",
    "def train_loso(model_type='baseline'):\n",
    "    \"\"\"Run full LOSO cross-validation.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Subject-Independent Evaluation (LOSO)\")\n",
    "    print(f\"  Model: {model_type}\")\n",
    "    print(f\"  Device: {DEVICE}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load all subject data\n",
    "    all_train, all_test, all_train_labels, all_test_labels = load_all_subjects(DATA_DIR)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for test_sub in range(N_SUBJECTS):\n",
    "        print(f\"\\n--- Testing on Subject {test_sub + 1} (trained on rest) ---\")\n",
    "        \n",
    "        # LOSO split\n",
    "        tr_X, tr_y, te_X, te_y = loso_split(\n",
    "            all_train, all_test, all_train_labels, all_test_labels, test_sub)\n",
    "        \n",
    "        train_ds = CombinedDataset(tr_X, tr_y)\n",
    "        test_ds = CombinedDataset(te_X, te_y)\n",
    "        \n",
    "        print(f\"  Train samples: {len(train_ds)}, Test samples: {len(test_ds)}\")\n",
    "        \n",
    "        # Create model\n",
    "        if model_type == 'efficient':\n",
    "            from efficient_eegencoder import EfficientEEGEncoder\n",
    "            model = EfficientEEGEncoder(n_classes=N_CLASSES).to(DEVICE)\n",
    "        else:\n",
    "            # Baseline needs lma.py â€” make sure EEGEncoder-main/ is in path\n",
    "            sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), 'EEGEncoder-main'))\n",
    "            from colab_train_baseline import EEGEncoder\n",
    "            model = EEGEncoder(n_classes=N_CLASSES).to(DEVICE)\n",
    "        \n",
    "        num_workers = 2 if DEVICE == 'cuda' else 0\n",
    "        train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True, num_workers=num_workers,\n",
    "                                        pin_memory=(DEVICE == 'cuda'))\n",
    "        test_loader = data.DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
    "                                       shuffle=False, num_workers=num_workers)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "        loss_func = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        scaler = torch.cuda.amp.GradScaler() if DEVICE == 'cuda' else None\n",
    "        \n",
    "        best_acc = 0\n",
    "        best_kappa = 0\n",
    "        \n",
    "        for epoch in range(N_EPOCHS):\n",
    "            # Train\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                \n",
    "                if scaler:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outs = model(inputs)\n",
    "                    loss = loss_func(outs, labels)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outs = model(inputs)\n",
    "                    loss = loss_func(outs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # Evaluate every 25 epochs\n",
    "            if (epoch + 1) % 25 == 0 or epoch == N_EPOCHS - 1:\n",
    "                model.eval()\n",
    "                preds, labels_list = [], []\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in test_loader:\n",
    "                        inputs = inputs.to(DEVICE)\n",
    "                        if scaler:\n",
    "                            with torch.cuda.amp.autocast():\n",
    "                                outs = model(inputs)\n",
    "                        else:\n",
    "                            outs = model(inputs)\n",
    "                        preds.extend(outs.argmax(-1).cpu().numpy().tolist())\n",
    "                        labels_list.extend(labels.argmax(-1).numpy().tolist())\n",
    "                \n",
    "                acc = accuracy_score(labels_list, preds)\n",
    "                kappa = cohen_kappa_score(labels_list, preds)\n",
    "                \n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_kappa = kappa\n",
    "                \n",
    "                print(f\"  Epoch {epoch+1:3d}/{N_EPOCHS} | Acc: {acc:.4f} | \"\n",
    "                      f\"Kappa: {kappa:.4f} | Best: {best_acc:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            'subject': test_sub + 1,\n",
    "            'accuracy': best_acc,\n",
    "            'kappa': best_kappa\n",
    "        })\n",
    "        print(f\"  âœ… Subject {test_sub+1}: Acc={best_acc:.4f}, Kappa={best_kappa:.4f}\")\n",
    "    \n",
    "    # Summary\n",
    "    mean_acc = np.mean([r['accuracy'] for r in results])\n",
    "    mean_kappa = np.mean([r['kappa'] for r in results])\n",
    "    std_acc = np.std([r['accuracy'] for r in results])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  SUBJECT-INDEPENDENT RESULTS ({model_type})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  {'Subject':<10} {'Accuracy':<12} {'Kappa':<10}\")\n",
    "    print(f\"  {'-'*32}\")\n",
    "    for r in results:\n",
    "        print(f\"  S{r['subject']:<9} {r['accuracy']:<12.4f} {r['kappa']:<10.4f}\")\n",
    "    print(f\"  {'-'*32}\")\n",
    "    print(f\"  {'Mean':<10} {mean_acc:<12.4f} {mean_kappa:<10.4f}\")\n",
    "    print(f\"  {'Std':<10} {std_acc:<12.4f}\")\n",
    "    print(f\"\\n  ðŸ“Š Paper SI accuracy: 74.48%\")\n",
    "    print(f\"  ðŸ“Š Your SI accuracy:  {mean_acc*100:.2f}% Â± {std_acc*100:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('--model', type=str, default='baseline',\n",
    "    #                     choices=['baseline', 'efficient'],\n",
    "    #                     help='Model type: baseline or efficient')\n",
    "    # args = parser.parse_args()\n",
    "    \n",
    "    train_loso(model_type='baseline')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
